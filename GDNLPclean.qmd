---
title: "NLPGD"
format:
  html:
    self-contained: true
editor: visual
---

## Preface

Employees are the heart and soul of a business. Their actions and performance are what drive the profitability of a business and it's ability to maximize shareholder value. In the following document, we look to address the following question:

*Is there a relationship between employee sentiment and excess returns?*

By building a webscraping program and applying it to GlassDoor, we were able to compile a comprehensive of employee reviews across a va of companies:

-   ExxonMobil (XOM)
-   Chevron (CVX)
-   Shell (SHEL)
-   BP (BP)

For context on the analysis that occurs below, here is a chart that represents their respective share prices from the last 10 years:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rollReg <- function (data, window, x, y) 
{
    rollReg_i <- function(data, window, x, y) {
        reg_output <- data.frame()
        for (i in window:nrow(data)) {
            subset <- data[(i - window + 1):i, ]
            model <- lm(formula = as.formula(paste0(y, "~", x)), 
                data = subset)
            slope <- summary(model)$coefficients[2]
            pVal <- stats::anova(model)$"Pr(>F)"[1]
            reg_output <- rbind(reg_output, data.frame(i = i, 
                slope = slope, pVal = pVal))
        }
        return(reg_output)
    }
    reg_output <- rollReg_i(data, window, x, y)
    result <- data %>% dplyr::mutate(i = row_number()) %>% dplyr::full_join(reg_output, 
        by = "i")
    return(result)
}

#credit: github:jvlahadamis/fintool
ezohlc <- function (tickers, start, end) 
{
    Symbol <- data <- mutated_data <- Date <- NULL
    all <- tidyquant::tq_get(x = tickers, get = "stock.prices", 
        from = start, to = end) %>% dplyr::rename_all(tools::toTitleCase) %>% 
        dplyr::group_by(Symbol) %>% tidyr::nest() %>% dplyr::mutate(mutated_data = purrr::map(.x = data, 
        .f = ~timetk::tk_xts(data = .x, date_var = Date) %>% 
            quantmod::adjustOHLC(.x, use.Adjusted = TRUE) %>% 
            timetk::tk_tbl(rename_index = "Date"))) %>% dplyr::select(-data) %>% 
        tidyr::unnest(mutated_data) %>% dplyr::rename(date = Date)
    return(all)
}

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(tidyquant)
library(plotly)
library(data.table)
library(lubridate)
library(zoo)
library(tidytext)
library(textdata)
library(plotly)
library(tidyr)
library(scales)
library(car)
library(gtsummary)
library(gt)
library(kableExtra)



bp <- read_csv("BP.csv") %>% 
  mutate(i = row_number())
chevron <- read_csv("chevron.csv") %>% 
  mutate(i = row_number())
exxon <- read_csv("exxonmobil.csv") %>% 
  mutate(i = row_number())
shell <- read_csv("Shell.csv") %>% 
  mutate(i = row_number())


# my os seems to import them as ...1, hence comment out
bp <- bp %>% 
  #select(-X)
  select(-`...1`)
shell <- shell %>%
  #select(-X)
  select(-`...1`)

# prep dataframe
ratings <- rbind(bp, shell, exxon, chevron)
ratings <- ratings %>%
  mutate('Company' = name1, 
         'reviewtext' = reviewBody,
         'Rating' = ratingValue,
         #lubridate method was giving a parsing error for 3, this way seems quiter
          date = as.Date(date, format = "%b %d, %Y")) %>%
          transmute(Company, name, Rating, reviewtext, date)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
tick <- c('SHEL', 'XOM', 'BP', 'CVX')

prices <- tidyquant::tq_get(tick,
                            get = 'stock.prices',
                            from = '2011-01-01',
                            to = Sys.Date())

price_plot <- prices %>% 
  rename(Price = adjusted,
         Ticker = symbol) %>% 
  ggplot(aes(x = date, y = Price, col = Ticker)) + 
  geom_line() +
  labs(title = 'Companies included in Analysis', y = "Price", x = "") + 
  theme_minimal()


ggplotly(price_plot)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# start playing around with sentiment
# afinn
afinn <- tidytext::get_sentiments('afinn')

afinn_nlp <- ratings %>%
  select(date, Company, reviewtext) %>%
  filter(date > "2016-01-01") %>% 
  unnest_tokens(output = word, input = reviewtext) %>% 
  inner_join(afinn) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(month, Company) %>%
  summarise(sentiment = mean(value), .groups = 'keep') 

#afinn_nlp %>% 
#  ggplot(aes(x = month, y = sentiment, col = Company)) + geom_bar(stat = 'identity') + facet_wrap(~Company)

#unsummarised base files used to compare models with different return windows
affin_base <- ratings %>%
  select(date, Company, reviewtext) %>% 
  unnest_tokens(output = word, input = reviewtext) %>% 
  inner_join(afinn) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  rename(company = Company)


```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# create sentiment analysis for 3 month rolling average
bing <- tidytext::get_sentiments('bing')

bing_nlp <- ratings %>%
  select(date, Company, reviewtext) %>%
  filter(date > "2016-01-01") %>%
  unnest_tokens(output = word, input = reviewtext) %>% 
  anti_join(stop_words) %>% 
  inner_join(bing) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>%
  group_by(month, Company, sentiment) %>%
  summarise(count = n(), .groups = 'keep') %>% 
  pivot_wider(names_from = sentiment, values_from = count, values_fill = list(count = 0)) %>% 
  mutate(total = positive + negative) %>% 
  mutate(percent_positive = (positive / total) * 100)
  #ggplot(aes(x = month, y = percent_positive, fill = Company)) +
  #geom_bar(stat = 'identity', position = 'stack') +
  #facet_wrap(~Company) + labs(y = 'Percentage Positive', x = '')\


bing_base <- ratings %>%
  select(date, Company, reviewtext) %>%
  unnest_tokens(output = word, input = reviewtext) %>% 
  anti_join(stop_words) %>% 
  inner_join(bing) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  rename(company = Company) %>% 
  mutate(p_n = case_when(sentiment == 'positive' ~ 1,
                   sentiment == 'negative' ~ 0,
                   T ~ NA))


rev_base <- rbind(bp, chevron, exxon, shell) %>% 
  transmute(ratingValue, name, reviewBody, company = name1, date, i) %>%
  mutate(date = as.Date(date, format = "%b %d, %Y")) %>%
  drop_na() %>% 
  arrange(company, date) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) 

```

We wanted to select companies that all operate in the same industry and differ by market cap. This ensures that our analysis truly represents differentiating factors in relation to excess returns.

Here is a chart showing how the review count changes through time, on a monthly basis. This is an important metric in how we selected our time frame for our analysis.
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ratings %>%  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(month, Company) %>% summarise(reviews = n()) %>% drop_na() %>% ggplot(aes(x = month, y = reviews, fill = Company)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~Company, scales = "free_y", ncol = 1) +
  theme_minimal() + 
  labs(x = "", y = "Count of Reviews", title = "Count of Reviews per Company per Month") +
  scale_fill_brewer(palette = "Set1") 
 
```
Note the small number of reviews during GlassDoors early years. For this reason, we filtered the data to be post 2013. 

Firstly, we needed to make sense of all the reviews and how best to compile and analyze them. 

#### **NLP**

Natural Language processing is the computational analysis of language and speech. This essentially analyzes words and phrases and rates them numerically or categorically based off of the emotion and message they are conveying. Here is a short description of the two methods we applied:

-   **Bing**: A dataset of 6,786 words with binary positive and negative sentiment scores

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(gt)
p <- bing %>% slice_head(n = 3) 
n <- bing %>% slice_tail(n = 3)
b <- rbind(p, n)
b %>% gt() %>% tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 20, heading.title.font.size = 30)
```

-   **Afinn**: A dataset of 2477 words with scores ranging from -5 to 5:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ap <- afinn %>% slice_head(n = 500)
an <- afinn %>% slice_tail(n = 500)
a <- rbind(ap,an) %>% arrange(desc(value)) %>% slice_head(n = 6)
a %>% gt() %>% tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 20, heading.title.font.size = 30)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Returns Data
ret <- prices %>%
  group_by(symbol) %>%
  mutate(returns = log(adjusted/lag(adjusted))) %>% 
  transmute(date, symbol, adjusted, returns)

tick <- c('^GSPC')

index <- tidyquant::tq_get(tick,
                           get = 'stock.prices',
                           from = '2011-01-01',
                           to = Sys.Date())

rf <- tidyquant::tq_get("DGS10",
                           get = "economic.data",
                           from = '2011-01-01',
                           to = Sys.Date()) %>% 
  transmute(date, rf = price/100) %>% 
  mutate(rf = ((rf + 1)^(1/252)) - 1) 


ret_index <- index %>%
  group_by(symbol) %>%
  mutate(returns = log(adjusted/lag(adjusted))) %>% 
  ungroup() %>% 
  transmute(date, sp500_close = adjusted, sp500_ret = returns)

#join rf to main returns; missing a few corresonding rf's, hence fill
ret_index <- full_join(rf, ret_index, by = 'date') %>%
  fill(rf, .direction = "down")
  
ret_join <- full_join(ret, ret_index, by = 'date') %>% 
  drop_na()


#div for roll reg
shell_ret <- ret_join %>% 
  filter(symbol == "SHEL")

exon_ret <- ret_join %>% 
  filter(symbol == "XOM")
  
bp_ret <- ret_join %>% 
  filter(symbol == "BP")

chev_ret <- ret_join %>% 
  filter(symbol == "CVX")

chev_ret <- rollReg(chev_ret, 252, 'sp500_ret', 'returns')

bp_ret <- rollReg(bp_ret, 252, 'sp500_ret', 'returns')

exon_ret <- rollReg(exon_ret, 252, 'sp500_ret', 'returns')

shell_ret <- rollReg(shell_ret, 252, 'sp500_ret', 'returns')

# alpha = Rasset - rf - beta*Rmarket + Beta*rf

ret_join_beta <- rbind(chev_ret, bp_ret, exon_ret, shell_ret)

ret_join_beta <- ret_join_beta %>% 
  #daily alpha
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>% 
  drop_na() %>%
  #y, q, m for summarising 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date)))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#old analysis; intial roll test

library(tidyverse)
library(zoo)
library(lubridate)

rateRollAvg <- rbind(bp, chevron, exxon, shell) %>% 
  transmute(ratingValue, name, reviewBody, company = name1, date, i) %>%
  mutate(date = as.Date(date, format = "%b %d, %Y")) %>%
  drop_na() %>% 
  arrange(company, date) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(company, month) %>%
  summarise(meanRateMon = mean(ratingValue)) %>% 
  group_by(company) %>%
  mutate(rollAvg3 = rollmean(meanRateMon, 3, fill = NA, align="right")) %>%
  mutate(rollAvg12 = rollmean(meanRateMon, 12, fill = NA, align="right")) %>% 
  drop_na() %>% 
  filter(month > "2015-12-31") %>% 
  mutate(xOver = case_when(rollAvg3 > rollAvg12 ~ 1,
                           rollAvg3 <= rollAvg12 ~ 0)) %>% 
  ungroup() %>% 
  arrange(month)


bing_nlp <- bing_nlp %>% 
  rename(company = Company)
afinn_nlp <- afinn_nlp %>% 
  rename(company = Company)

review_comb <- full_join(rateRollAvg, bing_nlp, by = c('month', 'company'))
review_comb <- full_join(review_comb, afinn_nlp, by = c('month', 'company'))


```


Just for fun, here is a wordcloud of different phrases:

```{r, fig.height = 8, fig.width = 10, echo=FALSE, message=FALSE, warning=FALSE}
library(wordcloud2)
ratings %>% 
  select(date, Company, reviewtext) %>%
  filter(date > "2013-12-31") %>% 
  unnest_tokens(output = word, input = reviewtext, token = 'ngrams', n = 4) %>% count(word, sort = T) %>% 
  drop_na() %>% wordcloud2()
```


Overwhelmingly positive. Need to think about the people leaving negative reviews after lashing out emotionally, then going back and deleting them out of guilt (or to save face as a lot of reviews contain their roles).



Even more fun, lets look at the negative ones: 
![image](C:/Users\conno\OneDrive\Desktop\Winter 24\FAT\cloud2.png)

This is just the high frequency occurances. If you scroll through a table of the lowest rated sentiment words, it gets quite entertaining: 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
ratings %>%
  select(date, Company, reviewtext) %>%
  unnest_tokens(output = word, input = reviewtext) %>%
  inner_join(afinn) %>%
  filter(value <= -3) %>%
  distinct(word, .keep_all = TRUE) %>%  gt() %>% tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 20, heading.title.font.size = 30) 

```








```{r, echo=FALSE, message=FALSE, warning=FALSE}
#chunk from initial analysis
tick <- c('SHEL', 'XOM', 'BP', 'CVX')

adj_price <- ezohlc(tick, '2013-01-01', Sys.Date())

retMon <- adj_price %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(month, Symbol) %>% 
  summarise(
    openMon = first(Open),
    closeMon = last(Close)) %>% 
  mutate(retMon = log(closeMon/openMon)) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil"))

# for binding to signal df; hypothesis is that indicators predict returns one month ahead. Thus, for model testing sake we need to subtract one month from return df to bind to indicator df
retMon4bind <- retMon %>% 
  mutate(month = (month + days(1)) - months(1) - days(1))

review_comb <- full_join(review_comb, retMon4bind, by = c('month', 'company'))
```

We also utilized the star ratings that employees leave whenever a review is posted (ranging from 1 star to 5 stars). To organize our data into workable metrics. We used individual words instead of ngrams (collection of words or short sentences) as the bulk of the reviews are short and sometimes contain only one word. To summarize, we opted for the following:

-   Grouped reviews into monthly, quarterly and annual bins.
-   Bing was organized into percentage of reviews that were deemed positive and averaged over monthly, quarterly and annual periods.
-   Afinn was organized into average total score over monthly, quarterly and annual periods.
-   Star ratings were averaged over monthly, quarterly and annual periods.

Here are some visuals that are separated by company, depicting how these various metrics change through time. All charts use monthly data to help tell the story:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
review_comb %>% plot_ly(x = ~month,
                    y = ~meanRateMon,
                    color = ~company,
                    type = 'scatter',
                    mode = 'lines') %>% layout(title = "Average Monthly Star Reviews",
         xaxis = list(title = ""),
         yaxis = list(title = "Stars"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
review_comb %>% filter(month >'2013-12-31') %>% 
  ggplot(aes(x = month, y = sentiment, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge", show.legend = FALSE) +
  facet_wrap(~ company, scales = "free_x") +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Afinn Analysis by Company and Date",
       x = "",
       y = "Sentiment Score") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
bing_nlp$month <- as.Date(bing_nlp$month)

ggplot(data = bing_nlp, aes(x = month)) +
  geom_bar(aes(y = negative, fill = 'negative'), stat = "identity") +
  geom_bar(aes(y = positive, fill = 'positive'), stat = "identity", alpha = 0.5) +
  geom_line(aes(y = percent_positive, group = company, color = company), size = 1) +
  facet_wrap(~ company, scales = "free_y") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "positive" = "green")) +
  scale_color_viridis_d() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(fill = "Review Sentiment", color = "Company",
       x = "", y = "Review Sentiment / Percent Positive")
```







# Alpha Analysis

-   Using CAPM, Alphas were calculated annually for each individual stock
    -   1 Year Beta
    -   S&P500 utilized as a proxy for market return
    -   10 Year U.S. Treasury security utilized as a proxy for the risk free rate
-   Annual Alphas were regressed annually against the prior year's average review ratings and written sentiment, as defined by the Affin dictionary
    -   Bing NLP excluded due to model multicollinearity

### Rational

-   Working with alpha instead of logarithmic returns captures returns that are not related to market-wide price movements
    -   Attempts to isolate returns to firm specific factors such as governance or commodity price exposure
-   Utilize sentiment indicators generated from the prior years review as a proxy to capture possible alpha derived from strong employee satisfaction
-   One year time frame captures returns and sentiment on a longer term horizon
    -   Attempt to better capture governance effects

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(patchwork)


ret_join_beta <- rbind(chev_ret, bp_ret, exon_ret, shell_ret)

ret_join_beta <- ret_join_beta %>% 
  #daily alpha
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>% 
  drop_na() %>%
  #y, q, m for summarising 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date)))

daily_alpha_plot <- ret_join_beta %>% 
  filter(date > '2013-12-31') %>% 
  ggplot(aes(x = date, y = alpha, col = symbol)) + geom_line() +
  labs(title = "Daily & Yearly Alpha",
       subtitle = "1 Year Rolling Beta",
       x = "",
       y = "Alpha",
       color = "Ticker") + 
  theme_minimal()

alpha_y <- ret_join_beta %>% 
  rename(Symbol = symbol) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup()


alpha_y <- alpha_y %>% 
  group_by(company) %>% 
  mutate(sp500_ret = log(sp500_close/lag(sp500_close)),
         returns = log(adjusted/lag(adjusted)),
         rf = ((rf + 1)^(252)) - 1) %>% 
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>%
  mutate(y = y - 1) %>% 
  drop_na() 


yearly_alpha_plot <- alpha_y %>% 
  ggplot(aes(x = date, y = alpha, col = company)) + geom_line() +
  labs(x = "",
       y = "") + 
  theme_minimal() +
  theme(legend.position = "none")



daily_alpha_plot/yearly_alpha_plot 

```

### Findings

Our sentiment proxy had no explanatory power when attempting to predict alpha returns in the proceeding year - Model fit was poor for all four companies - No model or factor significance was found - Counter-intuitive beta directionality

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#reviews as a predictor of alpha, leading 1 year (y)
rev_y <- rev_base %>% 
  group_by(y, company) %>% 
  summarise(mean_rating = mean(ratingValue), count = n()) 

affin_y <- affin_base %>% 
  group_by(company, y) %>% 
  summarise(sentiment = mean(value))

bing_y <- bing_base %>% 
  group_by(company, y) %>% 
  summarise(prop_postive = mean(p_n))
  
indicators_y <- full_join(rev_y, affin_y, by = c("company", "y"))
indicators_y <- full_join(bing_y, indicators_y, by = c("company", "y"))


alpha_y <- ret_join_beta %>% 
  filter(date >= '2014-12-31',
         date <= '2024-01-01') %>% 
  rename(Symbol = symbol) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  ungroup() %>% 
  select(-Symbol) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup()

alpha_y <- alpha_y %>% 
  group_by(company) %>% 
  mutate(sp500_ret = log(sp500_close/lag(sp500_close)),
         returns = log(adjusted/lag(adjusted)),
         rf = ((rf + 1)^(252)) - 1) %>% 
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>%
  mutate(y = y - 1) %>% 
  drop_na() 


comb_y <- full_join(alpha_y, indicators_y, by = c("company", "y")) %>% 
  transmute(y, date, company, alpha, prop_postive, mean_rating, sentiment) %>% 
  drop_na()

comb_chev <- comb_y %>% 
  filter(company == "Chevron")


model_chev <- lm(alpha ~ mean_rating + sentiment, data = comb_chev)

library(plotly)

mean_rating_seq <- seq(min(comb_chev$mean_rating), max(comb_chev$mean_rating), length.out = 100)
sentiment_seq <- seq(min(comb_chev$sentiment), max(comb_chev$sentiment), length.out = 100)

grid <- expand.grid(mean_rating = mean_rating_seq, sentiment = sentiment_seq)

grid$alpha_pred <- predict(model_chev, newdata = grid)

fig <- plot_ly(data = comb_chev, x = ~mean_rating, y = ~sentiment, z = ~alpha, type = 'scatter3d', mode = 'markers',
               marker = list(size = 5, opacity = 0.5))

fig <- fig %>%
  add_surface(x = mean_rating_seq, y = sentiment_seq, z = matrix(grid$alpha_pred, nrow = 100, ncol = 100), opacity = 0.5) %>%
  layout(title = list(text = "Chevron Multivariate Regression"),
    scene = list(xaxis = list(title = 'Mean Rating'),
                      yaxis = list(title = 'Sentiment'),
                      zaxis = list(title = 'Alpha')))
fig

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#summary(model_chev)

#vif(model_chev)
comb_bp <- comb_y %>% 
  filter(company == "bp")

model_bp <- lm(alpha ~ mean_rating + sentiment, data = comb_bp)

#summary(model_bp)

#vif(model_bp)

comb_exxon <- comb_y %>% 
  filter(company == "ExxonMobil")

model_exxon <- lm(alpha ~ mean_rating + sentiment, data = comb_exxon)

#vif(model_exxon)

#summary(model_exxon)

comb_shell <- comb_y %>% 
  filter(company == "Shell")

model_shell <- lm(alpha ~ mean_rating + sentiment, data = comb_shell)

#summary(model_shell)

#vif(model_shell)

wide_alpha <- alpha_y %>%
  ungroup() %>% 
  transmute(company, date, alpha) %>%  
  pivot_wider(names_from = company, values_from = alpha)  %>% 
  select(-date)

table_chev <- tbl_regression(model_chev)
table_bp <- tbl_regression(model_bp)
table_exxon <- tbl_regression(model_exxon)
table_shell <- tbl_regression(model_shell)

tbl_merge(
  list(table_chev, table_bp, table_exxon, table_shell),
  tab_spanner = c("Chevron", "BP", "ExxonMobil", "Shell"))


```

# Portfolio-based Analysis

Two portfolios of our selected companies were created:

-   Portflio 1: Naive equally-weighted portfolio
    -   Rebalanced yearly based upon the prior years closing price
-   Portfolio 2: Optimized portfolio utilizing three sentiment indicators
    -   Generated weights utilizing a combination the input indicators
    -   Optimization process to determine the ideal relative sentiment mix
        -   Training window: 2014-2021
    -   Rebalanced annually using an aggregation sentiment data from the prior year
        -   Sentiment is treated on a absolute basis as opposed to on a relative basis to prior years
        -   Companies are proportional de-weighted in cases of 'review slippage' relative to their peers
            -   No additionally compounds are included for +/- review drift

### Rational

-   Firms with consistently superior Glassdoor reviews, as measured by our sentiment indicators, will be receive more weight
    -   Shell maintained relatively superior reviews over the analysis window
    -   Attempts to capture excess returns generated by superior employee sentiment
-   Naive equally weighted portfolio intends to serve as a baseline
-   Captures returns on longer term horizon

```{r, echo=FALSE, message=FALSE, warning=FALSE}

port_y <- adj_price %>% 
  filter(date >= '2013-12-31',
         date <= '2024-01-01') %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup() %>% 
  #subtract a year to bind correct lagged sentiment
  mutate(y = y - 1) %>% 
  group_by(company) %>% 
  mutate(lag_close = lag(Close)) %>% 
  drop_na()

rev_y <- rev_base %>% 
  group_by(y, company) %>% 
  summarise(mean_rating = mean(ratingValue), count = n()) 

affin_y <- affin_base %>% 
  group_by(company, y) %>% 
  summarise(sentiment = mean(value))

bing_y <- bing_base %>% 
  group_by(company, y) %>% 
  summarise(prop_postive = mean(p_n))
  
indicators_y <- full_join(rev_y, affin_y, by = c("company", "y"))
indicators_y <- full_join(bing_y, indicators_y, by = c("company", "y"))

#joining prices at year end with sentiment
port_y <- full_join(port_y, indicators_y, by = c("company", "y"))  %>% 
  select(-count)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}


port_y_train <- port_y %>% 
  filter(date < '2020-12-31')

trade_func <- function(data, prop, rate, sent) {
  port_y_norm <- data %>% 
  #normalizing indicators
  mutate(mean_rating = mean_rating,
         sentiment = sentiment) %>% 
  transmute(date, company, close = Close, y, prop_postive, mean_rating, sentiment, lag_close) %>% 
  drop_na() %>% 
  mutate(comb_sent = prop*prop_postive + rate*mean_rating + sent*sentiment,
         price_norm = 1 / lag_close) %>% 
  group_by(y) %>% 
  #weighted sentiment
  mutate(weight_sent = comb_sent / sum(comb_sent)) %>%
  #adj for close prices in the prior year
  mutate(weight = weight_sent*price_norm/sum(price_norm*weight_sent)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
  weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) %>% 
  mutate(prop = prop, 
         rate = rate,
         sent = sent)
  
    return(weighted_returns) 
  }


out <- expand.grid(
  prop = seq(from = 0, to = 1, by = .02),
  rate = seq(from = 0, to = 1, by = .02),
  sent = seq(from = 0, to = 1, by = .02)) %>% 
  mutate(total = prop + rate + sent) %>% 
  filter(total == 1)
  

library(foreach)
library(doParallel)

n_cores <- detectCores() - 1

cl <- makeCluster(n_cores)
registerDoParallel(cl)

res <- foreach(
  combo = iter(out, by = 'row'), 
  .combine = rbind,
  .packages = c("dplyr", "tidyverse", "tidyquant", "PerformanceAnalytics") 
) %dopar% {

  prop <- combo$prop
  rate <- combo$rate
  sent <- combo$sent
  
 
  trade_func(data = port_y_train, prop = prop, rate = rate, sent = sent)
}


stopCluster(cl)




rebal_func <- function(data) {
  port_y_norm <- data %>% 
  transmute(date, company, close = Close, y, lag_close) %>% 
  drop_na() %>% 
  mutate(price_norm = 1 / lag_close) %>% 
  group_by(y) %>%
  #adj for close prices in the prior year
  mutate(weight = price_norm/sum(price_norm)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
  weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) 
  
    return(weighted_returns) 
  }


sentiment = unique(res$sent)
rate = unique(out$rate)
LogReturn <-
  res %>% dplyr::select(sent, rate, logRet_sum) %>%
  tidyr::pivot_wider(values_from = logRet_sum, names_from = rate) %>%
  dplyr::select(-1) %>% as.matrix()
plot_ly(x = ~ sentiment,
        y = ~ rate,
        z = ~ LogReturn) %>% add_surface() %>%
  layout(title = list(text = "Optimization Result - Training"))

```

### Findings

Utilizing our sentiment indicators, we were not able to generate a portfolio that outperformed our baseline during the test period

-   Test window (2020-2023) suggests superior returns observed in the training window were spurious

```{r, echo=FALSE, message=FALSE, warning=FALSE}

sent_ret <- trade_func(port_y, 0, 0, 1) 

naive_ret <- rebal_func(port_y) 

naive_ret$prop <- NA
naive_ret$rate <- NA
naive_ret$sent <- NA


comb_ret_port <- rbind(
  mutate(sent_ret, Portfolio = "Optimized Sentiment-based"),
  mutate(naive_ret , Portfolio = "Naive")) %>% 
  rename(`Positive Prop.` = prop,
         Rating = rate,
         Sentiment = sent,
         Variance = var,
         Return = logRet_sum)


kable(comb_ret_port, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(1, bold = TRUE) %>% 
  add_header_above(c(" " = 2, "Optimized Weights" = 3, " " = 1))
  


port_y_norm <- port_y %>% 
  #normalizing indicators
  mutate(mean_rating = mean_rating/5,
         sentiment = sentiment/5) %>% 
  transmute(date, company, close = Close, y, prop_postive, mean_rating, sentiment, lag_close) %>% 
  drop_na() %>% 
  mutate(comb_sent = 0*prop_postive + 0*mean_rating + 1*sentiment,
         price_norm = 1 / lag_close) %>% 
  group_by(y) %>% 
  #weighted sentiment
  mutate(weight_sent = comb_sent / sum(comb_sent)) %>%
  #adj for close prices in the prior year
  mutate(weight = weight_sent*price_norm/sum(price_norm*weight_sent)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2014-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)

  
weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) %>% 
  mutate(prop = 0, 
         rate = 0,
         sent = 1)
  

  
port_rebal <- port_y %>% 
  transmute(date, company, close = Close, y, lag_close) %>% 
  drop_na() %>% 
  mutate(price_norm = 1 / lag_close) %>% 
  group_by(y) %>%
  #adj for close prices in the prior year
  mutate(weight = price_norm/sum(price_norm)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
weighted_returns <- port_rebal %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) 
    
  
weighted_close <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value)))
  
  
weighted_close_rebal <- port_rebal %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value)))%>%
  group_by(date) %>% 
  summarise(port_value = sum(port_value))
  
portVal_plot <- weighted_close %>% 
  mutate(Year = year(date)) %>%
  group_by(Year) %>% 
  summarise(Sentiment = sum(port_value)) %>% 
  mutate(`Naive` = weighted_close_rebal$port_value) %>% 
  pivot_longer(-Year, names_to = "Portfolio", values_to = "Value") %>% 
  ggplot(aes(y = Value, x = Year, col = Portfolio)) + geom_line() +
  labs(x = "", y = "Porfolio Value", title = "Portfolio Comparison (Test & Train)", subtitle = "Training Window: 2014 - 2020") + theme_minimal()
                    

ggplotly(portVal_plot)                              

```

### Review/Next Steps 
The way in which we have captured sentiment and bound our review data to return data (i.e by year) has very little utility in terms of creating some sort of predicative model. It is clear that external firm specific factors have a much larger impact, such as commodity prices (although this is unsurprising). Still, we beilieve that high frequency GlassDoor reviews when combined with NLP have the potential to capture employee sentiment to proxy against excess returns. 

* Dynamics of sentiment trend corresponds with the economic environment, seemingly do contain trends in periods of large market swings and contain components that point to the fact that they are more than white noise. 
* Further analysis would include a much broader subset of companies ideally with a higher number of reviews. 
* We were constrained by the run time of our webscraping program. 
* We would like to explore with different sectors (maybe there is a bigger impact in tech?).
* Explore various machine learning techniques to see if there is model improvement. 
