---
title: "NLPGD"
format:
  html:
    self-contained: true
editor: visual
---
## Preface

Employees are the heart and soul of a business. Their actions and performance are what drive the profitability of a business and it's ability to maximize shareholder value. In the following document, we look to address the following question:

*Is there a relationship between employee sentiment and excess returns?*

By building a webscraping program and applying it to GlassDoor, we were able to compile a comprehensive of employee reviews across a va of companies:

-   ExxonMobil (XOM)
-   Chevron (CVX)
-   Shell (SHEL)
-   BP (BP)

For context on the analysis that occurs below, here is a chart that represents their respective share prices from the last 10 years:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
rollReg <- function (data, window, x, y) 
{
    rollReg_i <- function(data, window, x, y) {
        reg_output <- data.frame()
        for (i in window:nrow(data)) {
            subset <- data[(i - window + 1):i, ]
            model <- lm(formula = as.formula(paste0(y, "~", x)), 
                data = subset)
            slope <- summary(model)$coefficients[2]
            pVal <- stats::anova(model)$"Pr(>F)"[1]
            reg_output <- rbind(reg_output, data.frame(i = i, 
                slope = slope, pVal = pVal))
        }
        return(reg_output)
    }
    reg_output <- rollReg_i(data, window, x, y)
    result <- data %>% dplyr::mutate(i = row_number()) %>% dplyr::full_join(reg_output, 
        by = "i")
    return(result)
}

#credit: github:jvlahadamis/fintool
ezohlc <- function (tickers, start, end) 
{
    Symbol <- data <- mutated_data <- Date <- NULL
    all <- tidyquant::tq_get(x = tickers, get = "stock.prices", 
        from = start, to = end) %>% dplyr::rename_all(tools::toTitleCase) %>% 
        dplyr::group_by(Symbol) %>% tidyr::nest() %>% dplyr::mutate(mutated_data = purrr::map(.x = data, 
        .f = ~timetk::tk_xts(data = .x, date_var = Date) %>% 
            quantmod::adjustOHLC(.x, use.Adjusted = TRUE) %>% 
            timetk::tk_tbl(rename_index = "Date"))) %>% dplyr::select(-data) %>% 
        tidyr::unnest(mutated_data) %>% dplyr::rename(date = Date)
    return(all)
}

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)
library(tidyquant)
library(plotly)
library(data.table)
library(lubridate)
library(zoo)
library(tidytext)
library(textdata)
library(plotly)
library(tidyr)
library(scales)
library(car)
library(gtsummary)
library(gt)
library(kableExtra)



bp <- read_csv("BP.csv") %>% 
  mutate(i = row_number())
chevron <- read_csv("chevron.csv") %>% 
  mutate(i = row_number())
exxon <- read_csv("exxonmobil.csv") %>% 
  mutate(i = row_number())
shell <- read_csv("Shell.csv") %>% 
  mutate(i = row_number())


# my os seems to import them as ...1, hence comment out
bp <- bp %>% 
  #select(-X)
  select(-`...1`)
shell <- shell %>%
  #select(-X)
  select(-`...1`)

# prep dataframe
ratings <- rbind(bp, shell, exxon, chevron)
ratings <- ratings %>%
  mutate('Company' = name1, 
         'reviewtext' = reviewBody,
         'Rating' = ratingValue,
         #lubridate method was giving a parsing error for 3, this way seems quiter
          date = as.Date(date, format = "%b %d, %Y")) %>%
          transmute(Company, name, Rating, reviewtext, date)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
tick <- c('SHEL', 'XOM', 'BP', 'CVX')

prices <- tidyquant::tq_get(tick,
                            get = 'stock.prices',
                            from = '2011-01-01',
                            to = Sys.Date())

price_plot <- prices %>% 
  rename(Price = adjusted,
         Ticker = symbol) %>% 
  ggplot(aes(x = date, y = Price, col = Ticker)) + 
  geom_line() +
  labs(title = 'Companies included in Analysis', y = "Price", x = "") + 
  theme_minimal()


ggplotly(price_plot)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# start playing around with sentiment
# afinn
afinn <- tidytext::get_sentiments('afinn')

afinn_nlp <- ratings %>%
  select(date, Company, reviewtext) %>%
  filter(date > "2016-01-01") %>% 
  unnest_tokens(output = word, input = reviewtext) %>% 
  inner_join(afinn) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(month, Company) %>%
  summarise(sentiment = mean(value), .groups = 'keep') 

#afinn_nlp %>% 
#  ggplot(aes(x = month, y = sentiment, col = Company)) + geom_bar(stat = 'identity') + facet_wrap(~Company)

#unsummarised base files used to compare models with different return windows
affin_base <- ratings %>%
  select(date, Company, reviewtext) %>% 
  unnest_tokens(output = word, input = reviewtext) %>% 
  inner_join(afinn) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  rename(company = Company)


```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# create sentiment analysis for 3 month rolling average
bing <- tidytext::get_sentiments('bing')

bing_nlp <- ratings %>%
  select(date, Company, reviewtext) %>%
  filter(date > "2016-01-01") %>%
  unnest_tokens(output = word, input = reviewtext) %>% 
  anti_join(stop_words) %>% 
  inner_join(bing) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>%
  group_by(month, Company, sentiment) %>%
  summarise(count = n(), .groups = 'keep') %>% 
  pivot_wider(names_from = sentiment, values_from = count, values_fill = list(count = 0)) %>% 
  mutate(total = positive + negative) %>% 
  mutate(percent_positive = (positive / total) * 100)
  #ggplot(aes(x = month, y = percent_positive, fill = Company)) +
  #geom_bar(stat = 'identity', position = 'stack') +
  #facet_wrap(~Company) + labs(y = 'Percentage Positive', x = '')\


bing_base <- ratings %>%
  select(date, Company, reviewtext) %>%
  unnest_tokens(output = word, input = reviewtext) %>% 
  anti_join(stop_words) %>% 
  inner_join(bing) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  rename(company = Company) %>% 
  mutate(p_n = case_when(sentiment == 'positive' ~ 1,
                   sentiment == 'negative' ~ 0,
                   T ~ NA))


rev_base <- rbind(bp, chevron, exxon, shell) %>% 
  transmute(ratingValue, name, reviewBody, company = name1, date, i) %>%
  mutate(date = as.Date(date, format = "%b %d, %Y")) %>%
  drop_na() %>% 
  arrange(company, date) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) 

```

We wanted to select companies that all operate in the same industry and differ by market cap. This ensures that our analysis truly represents differentiating factors in relation to excess returns.

Firstly, we needed to make sense of all the reviews and how best to compile and analyze them. Natural Language processing is the computational analysis of language and speech. This essentially analyzes words and phrases and rates them numerically or categorically based off of the emotion and message they are conveying. Here is a short description of the two methods we applied:

-   Bing: A dataset of 6,786 words with binary positive and negative sentiment scores
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(gt)
p <- bing %>% slice_head(n = 3) 
n <- bing %>% slice_tail(n = 3)
b <- rbind(p, n)
b %>% gt() %>% tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 10, heading.title.font.size = 15)
```
-   Afinn: A dataset of 2477 words with scores ranging from -5 to 5:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ap <- afinn %>% slice_head(n = 500)
an <- afinn %>% slice_tail(n = 500)
a <- rbind(ap,an) %>% arrange(desc(value)) %>% slice_head(n = 6)
a %>% gt() %>% tab_options(
  data_row.padding = px(6),
  heading.align = 'left',
  column_labels.background.color = 'dodgerblue4',
  heading.title.font.size = 26,
  footnotes.font.size = 8
) %>% 
  tab_style(style = cell_text(color = 'dodgerblue4',
                              weight = 'bold'),
            locations = cells_title(groups = 'title')) %>% tab_options(table.font.size = 10, heading.title.font.size = 15)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Returns Data
ret <- prices %>%
  group_by(symbol) %>%
  mutate(returns = log(adjusted/lag(adjusted))) %>% 
  transmute(date, symbol, adjusted, returns)

tick <- c('^GSPC')

index <- tidyquant::tq_get(tick,
                           get = 'stock.prices',
                           from = '2011-01-01',
                           to = Sys.Date())

rf <- tidyquant::tq_get("DGS10",
                           get = "economic.data",
                           from = '2011-01-01',
                           to = Sys.Date()) %>% 
  transmute(date, rf = price/100) %>% 
  mutate(rf = ((rf + 1)^(1/252)) - 1) 


ret_index <- index %>%
  group_by(symbol) %>%
  mutate(returns = log(adjusted/lag(adjusted))) %>% 
  ungroup() %>% 
  transmute(date, sp500_close = adjusted, sp500_ret = returns)

#join rf to main returns; missing a few corresonding rf's, hence fill
ret_index <- full_join(rf, ret_index, by = 'date') %>%
  fill(rf, .direction = "down")
  
ret_join <- full_join(ret, ret_index, by = 'date') %>% 
  drop_na()


#div for roll reg
shell_ret <- ret_join %>% 
  filter(symbol == "SHEL")

exon_ret <- ret_join %>% 
  filter(symbol == "XOM")
  
bp_ret <- ret_join %>% 
  filter(symbol == "BP")

chev_ret <- ret_join %>% 
  filter(symbol == "CVX")

chev_ret <- rollReg(chev_ret, 252, 'sp500_ret', 'returns')

bp_ret <- rollReg(bp_ret, 252, 'sp500_ret', 'returns')

exon_ret <- rollReg(exon_ret, 252, 'sp500_ret', 'returns')

shell_ret <- rollReg(shell_ret, 252, 'sp500_ret', 'returns')

# alpha = Rasset - rf - beta*Rmarket + Beta*rf

ret_join_beta <- rbind(chev_ret, bp_ret, exon_ret, shell_ret)

ret_join_beta <- ret_join_beta %>% 
  #daily alpha
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>% 
  drop_na() %>%
  #y, q, m for summarising 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date)))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#old analysis; intial roll test

library(tidyverse)
library(zoo)
library(lubridate)

rateRollAvg <- rbind(bp, chevron, exxon, shell) %>% 
  transmute(ratingValue, name, reviewBody, company = name1, date, i) %>%
  mutate(date = as.Date(date, format = "%b %d, %Y")) %>%
  drop_na() %>% 
  arrange(company, date) %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(company, month) %>%
  summarise(meanRateMon = mean(ratingValue)) %>% 
  group_by(company) %>%
  mutate(rollAvg3 = rollmean(meanRateMon, 3, fill = NA, align="right")) %>%
  mutate(rollAvg12 = rollmean(meanRateMon, 12, fill = NA, align="right")) %>% 
  drop_na() %>% 
  filter(month > "2015-12-31") %>% 
  mutate(xOver = case_when(rollAvg3 > rollAvg12 ~ 1,
                           rollAvg3 <= rollAvg12 ~ 0)) %>% 
  ungroup() %>% 
  arrange(month)


bing_nlp <- bing_nlp %>% 
  rename(company = Company)
afinn_nlp <- afinn_nlp %>% 
  rename(company = Company)

review_comb <- full_join(rateRollAvg, bing_nlp, by = c('month', 'company'))
review_comb <- full_join(review_comb, afinn_nlp, by = c('month', 'company'))


```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#chunk from initial analysis
tick <- c('SHEL', 'XOM', 'BP', 'CVX')

adj_price <- ezohlc(tick, '2013-01-01', Sys.Date())

retMon <- adj_price %>%
  mutate(month = (ceiling_date(date, unit = 'month') - days(1))) %>% 
  group_by(month, Symbol) %>% 
  summarise(
    openMon = first(Open),
    closeMon = last(Close)) %>% 
  mutate(retMon = log(closeMon/openMon)) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil"))

# for binding to signal df; hypothesis is that indicators predict returns one month ahead. Thus, for model testing sake we need to subtract one month from return df to bind to indicator df
retMon4bind <- retMon %>% 
  mutate(month = (month + days(1)) - months(1) - days(1))

review_comb <- full_join(review_comb, retMon4bind, by = c('month', 'company'))
```
We also utilized the star ratings that employees leave whenever a review is posted (ranging from 1 star to 5 stars). To organize our data into workable metrics. We used individual words instead of ngrams (collection of words or short sentences) as the bulk of the reviews are short and sometimes contain only one word. To summarize, we opted for the following:

-   Grouped reviews into monthly, quarterly and annual bins.
-   Bing was organized into percentage of reviews that were deemed positive and averaged over monthly, quarterly and annually.
-   Afinn was organized into average total score over monthly, quarterly and annually.
-   Star ratings were averaged over monthly, quarterly and annually.

Here are some visuals that are separated by company, depicting how these various metrics change through time. All charts use monthly data to help tell the story:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
review_comb %>% plot_ly(x = ~month,
                    y = ~meanRateMon,
                    color = ~company,
                    type = 'scatter',
                    mode = 'lines') %>% layout(title = "Average Monthly Star Reviews",
         xaxis = list(title = ""),
         yaxis = list(title = "Stars"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
review_comb %>% filter(month >'2013-12-31') %>% 
  ggplot(aes(x = month, y = sentiment, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge", show.legend = FALSE) +
  facet_wrap(~ company, scales = "free_x") +
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Afinn Analysis by Company and Date",
       x = "",
       y = "Sentiment Score") +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ratings %>%
  select(date, Company, reviewtext) %>%
  filter(date > "2013-12-31") %>% 
  unnest_tokens(output = word, input = reviewtext) %>%  inner_join(afinn) %>% filter(Company == 'bp') %>% 
  filter(date > '2018-01-01' & date < '2018-01-31')
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
bing_nlp$month <- as.Date(bing_nlp$month)

ggplot(data = bing_nlp, aes(x = month)) +
  geom_bar(aes(y = negative, fill = 'negative'), stat = "identity") +
  geom_bar(aes(y = positive, fill = 'positive'), stat = "identity", alpha = 0.5) +
  geom_line(aes(y = percent_positive, group = company, color = company), size = 1) +
  facet_wrap(~ company, scales = "free_y") +
  theme_minimal() +
  scale_fill_manual(values = c("negative" = "red", "positive" = "green")) +
  scale_color_viridis_d() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(fill = "Review Sentiment", color = "Company",
       x = "", y = "Review Sentiment / Percent Positive")

```



# Alpha Analysis
- Using CAPM, Alphas were calculated annually for each individual stock
  - 1 Year Beta
  - S&P500 utilized as a proxy for market return
  
- Annual Alphas were regressed annually against the preceding year's average review ratings and sentiment, as defined by the Affin dictionary.
  - Bing NLP excluded due to model multicollinearity

- Model fit was poor for all four companies

- Alpha between the four firms w
  
```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(patchwork)


ret_join_beta <- rbind(chev_ret, bp_ret, exon_ret, shell_ret)

ret_join_beta <- ret_join_beta %>% 
  #daily alpha
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>% 
  drop_na() %>%
  #y, q, m for summarising 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date)))

daily_alpha_plot <- ret_join_beta %>% 
  filter(date > '2013-12-31') %>% 
  ggplot(aes(x = date, y = alpha, col = symbol)) + geom_line() +
  labs(title = "Daily & Yearly Alpha",
       subtitle = "1 Year Rolling Beta",
       x = "",
       y = "Alpha",
       color = "Ticker") + 
  theme_minimal()

alpha_y <- ret_join_beta %>% 
  rename(Symbol = symbol) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup()


alpha_y <- alpha_y %>% 
  group_by(company) %>% 
  mutate(sp500_ret = log(sp500_close/lag(sp500_close)),
         returns = log(adjusted/lag(adjusted)),
         rf = ((rf + 1)^(252)) - 1) %>% 
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>%
  mutate(y = y - 1) %>% 
  drop_na() 


yearly_alpha_plot <- alpha_y %>% 
  ggplot(aes(x = date, y = alpha, col = company)) + geom_line() +
  labs(x = "",
       y = "") + 
  theme_minimal() +
  theme(legend.position = "none")



daily_alpha_plot/yearly_alpha_plot 

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

#reviews as a predictor of alpha, leading 1 year (y)
rev_y <- rev_base %>% 
  group_by(y, company) %>% 
  summarise(mean_rating = mean(ratingValue), count = n()) 

affin_y <- affin_base %>% 
  group_by(company, y) %>% 
  summarise(sentiment = mean(value))

bing_y <- bing_base %>% 
  group_by(company, y) %>% 
  summarise(prop_postive = mean(p_n))
  
indicators_y <- full_join(rev_y, affin_y, by = c("company", "y"))
indicators_y <- full_join(bing_y, indicators_y, by = c("company", "y"))


alpha_y <- ret_join_beta %>% 
  filter(date >= '2014-12-31',
         date <= '2024-01-01') %>% 
  rename(Symbol = symbol) %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  ungroup() %>% 
  select(-Symbol) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup()

alpha_y <- alpha_y %>% 
  group_by(company) %>% 
  mutate(sp500_ret = log(sp500_close/lag(sp500_close)),
         returns = log(adjusted/lag(adjusted)),
         rf = ((rf + 1)^(252)) - 1) %>% 
  mutate(alpha = returns - rf - slope*sp500_ret + slope*rf) %>%
  mutate(y = y - 1) %>% 
  drop_na() 


comb_y <- full_join(alpha_y, indicators_y, by = c("company", "y")) %>% 
  transmute(y, date, company, alpha, prop_postive, mean_rating, sentiment) %>% 
  drop_na()

comb_chev <- comb_y %>% 
  filter(company == "Chevron")


model_chev <- lm(alpha ~ mean_rating + sentiment, data = comb_chev)

library(plotly)

mean_rating_seq <- seq(min(comb_chev$mean_rating), max(comb_chev$mean_rating), length.out = 100)
sentiment_seq <- seq(min(comb_chev$sentiment), max(comb_chev$sentiment), length.out = 100)

grid <- expand.grid(mean_rating = mean_rating_seq, sentiment = sentiment_seq)

grid$alpha_pred <- predict(model_chev, newdata = grid)

fig <- plot_ly(data = comb_chev, x = ~mean_rating, y = ~sentiment, z = ~alpha, type = 'scatter3d', mode = 'markers',
               marker = list(size = 5, opacity = 0.5))

fig <- fig %>%
  add_surface(x = mean_rating_seq, y = sentiment_seq, z = matrix(grid$alpha_pred, nrow = 100, ncol = 100), opacity = 0.5) %>%
  layout(scene = list(xaxis = list(title = 'Mean Rating'),
                      yaxis = list(title = 'Sentiment'),
                      zaxis = list(title = 'Alpha')))
fig

#summary(model_chev)

#vif(model_chev)
comb_bp <- comb_y %>% 
  filter(company == "bp")

model_bp <- lm(alpha ~ mean_rating + sentiment, data = comb_bp)

#summary(model_bp)

#vif(model_bp)

comb_exxon <- comb_y %>% 
  filter(company == "ExxonMobil")

model_exxon <- lm(alpha ~ mean_rating + sentiment, data = comb_exxon)

#vif(model_exxon)

#summary(model_exxon)

comb_shell <- comb_y %>% 
  filter(company == "Shell")

model_shell <- lm(alpha ~ mean_rating + sentiment, data = comb_shell)

#summary(model_shell)

#vif(model_shell)

wide_alpha <- alpha_y %>%
  ungroup() %>% 
  transmute(company, date, alpha) %>%  
  pivot_wider(names_from = company, values_from = alpha)  %>% 
  select(-date)

table_chev <- tbl_regression(model_chev)
table_bp <- tbl_regression(model_bp)
table_exxon <- tbl_regression(model_exxon)
table_shell <- tbl_regression(model_shell)

tbl_merge(
  list(table_chev, table_bp, table_exxon, table_shell),
  tab_spanner = c("Chevron", "BP", "ExxonMobil", "Shell"))


```

# Portfolio-based Analysis
#### Two portfolios of our selected companies were created

- Portflio 1: Naive equally-weighted portfolio
  - Rebalanced yearly based upon the prior years closing price

- Portfolio 2: Optimized portfolio utilizing three sentiment indicators
  - Generated weights utilizing a combination the input indicators
  - Optimization process to determine the ideal relative sentiment mix
    - Training window: 2014-2021
  - Rebalanced annually using an aggregation sentiment data from the prior year
    - Sentiment is treated on a absolute basis as opposed to on a relative basis to prior years
    - Companies are proportional de-weighted in cases of 'review slippage' relative to their peers
      - No additionally compounds are included for +/- review drift 



```{r, echo=FALSE, message=FALSE, warning=FALSE}

port_y <- adj_price %>% 
  filter(date >= '2013-12-31',
         date <= '2024-01-01') %>% 
  mutate(company = case_when(Symbol == "BP" ~ "bp",
                             Symbol == "CVX" ~ "Chevron",
                             Symbol == "SHEL" ~ "Shell",
                             Symbol == "XOM" ~ "ExxonMobil")) %>% 
  mutate(y = year(date),
         q = paste0(y, quarter(date)),
         m = paste0(y, month(date))) %>% 
  group_by(y) %>% 
  filter(date == max(date)) %>% 
  ungroup() %>% 
  #subtract a year to bind correct lagged sentiment
  mutate(y = y - 1) %>% 
  group_by(company) %>% 
  mutate(lag_close = lag(Close)) %>% 
  drop_na()

rev_y <- rev_base %>% 
  group_by(y, company) %>% 
  summarise(mean_rating = mean(ratingValue), count = n()) 

affin_y <- affin_base %>% 
  group_by(company, y) %>% 
  summarise(sentiment = mean(value))

bing_y <- bing_base %>% 
  group_by(company, y) %>% 
  summarise(prop_postive = mean(p_n))
  
indicators_y <- full_join(rev_y, affin_y, by = c("company", "y"))
indicators_y <- full_join(bing_y, indicators_y, by = c("company", "y"))

#joining prices at year end with sentiment
port_y <- full_join(port_y, indicators_y, by = c("company", "y"))  %>% 
  select(-count)

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}


port_y_train <- port_y %>% 
  filter(date < '2020-12-31')

trade_func <- function(data, prop, rate, sent) {
  port_y_norm <- data %>% 
  #normalizing indicators
  mutate(mean_rating = mean_rating,
         sentiment = sentiment) %>% 
  transmute(date, company, close = Close, y, prop_postive, mean_rating, sentiment, lag_close) %>% 
  drop_na() %>% 
  mutate(comb_sent = prop*prop_postive + rate*mean_rating + sent*sentiment,
         price_norm = 1 / lag_close) %>% 
  group_by(y) %>% 
  #weighted sentiment
  mutate(weight_sent = comb_sent / sum(comb_sent)) %>%
  #adj for close prices in the prior year
  mutate(weight = weight_sent*price_norm/sum(price_norm*weight_sent)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
  weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) %>% 
  mutate(prop = prop, 
         rate = rate,
         sent = sent)
  
    return(weighted_returns) 
  }


out <- expand.grid(
  prop = seq(from = 0, to = 1, by = .02),
  rate = seq(from = 0, to = 1, by = .02),
  sent = seq(from = 0, to = 1, by = .02)) %>% 
  mutate(total = prop + rate + sent) %>% 
  filter(total == 1)
  

library(foreach)
library(doParallel)

n_cores <- detectCores() - 1

cl <- makeCluster(n_cores)
registerDoParallel(cl)

res <- foreach(
  combo = iter(out, by = 'row'), 
  .combine = rbind,
  .packages = c("dplyr", "tidyverse", "tidyquant", "PerformanceAnalytics") 
) %dopar% {

  prop <- combo$prop
  rate <- combo$rate
  sent <- combo$sent
  
 
  trade_func(data = port_y_train, prop = prop, rate = rate, sent = sent)
}


stopCluster(cl)




rebal_func <- function(data) {
  port_y_norm <- data %>% 
  transmute(date, company, close = Close, y, lag_close) %>% 
  drop_na() %>% 
  mutate(price_norm = 1 / lag_close) %>% 
  group_by(y) %>%
  #adj for close prices in the prior year
  mutate(weight = price_norm/sum(price_norm)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
  weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) 
  
    return(weighted_returns) 
  }




sent_ret <- trade_func(port_y, 1, 0, 0) 

naive_ret <- rebal_func(port_y) 

naive_ret$prop <- NA
naive_ret$rate <- NA
naive_ret$sent <- NA


comb_ret_port <- rbind(
  mutate(sent_ret, Portfolio = "Optimized Sentiment-based"),
  mutate(naive_ret , Portfolio = "Naive")) %>% 
  rename(`Positive Prop.` = prop,
         Rating = rate,
         Sentiment = sent,
         Variance = var,
         Return = logRet_sum)


kable(comb_ret_port, "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  column_spec(1, bold = TRUE) %>% 
  add_header_above(c(" " = 2, "Optimized Weights" = 3, " " = 1))
  


port_y_norm <- port_y %>% 
  #normalizing indicators
  mutate(mean_rating = mean_rating/5,
         sentiment = sentiment/5) %>% 
  transmute(date, company, close = Close, y, prop_postive, mean_rating, sentiment, lag_close) %>% 
  drop_na() %>% 
  mutate(comb_sent = 0*prop_postive + 0*mean_rating + 1*sentiment,
         price_norm = 1 / lag_close) %>% 
  group_by(y) %>% 
  #weighted sentiment
  mutate(weight_sent = comb_sent / sum(comb_sent)) %>%
  #adj for close prices in the prior year
  mutate(weight = weight_sent*price_norm/sum(price_norm*weight_sent)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2014-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)

  
weighted_returns <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) %>% 
  mutate(prop = 0, 
         rate = 0,
         sent = 1)
  

  
port_rebal <- port_y %>% 
  transmute(date, company, close = Close, y, lag_close) %>% 
  drop_na() %>% 
  mutate(price_norm = 1 / lag_close) %>% 
  group_by(y) %>%
  #adj for close prices in the prior year
  mutate(weight = price_norm/sum(price_norm)) %>%
  arrange(company, date) %>%
  group_by(company) %>% 
  mutate(previous_weight = lag(weight), # previous weight
         trades = case_when(
           date == '2015-12-31' ~ 1, # Starting strategy
           is.na(previous_weight) ~ NA_real_, # Handle the first row 
           weight < previous_weight ~ -1, # Selling
           weight > previous_weight ~ 1, # Buying
           TRUE ~ 0 # No change
         )) %>%
  ungroup() %>% 
  mutate(position = 4)
  
weighted_returns <- port_rebal %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value))) %>% 
  drop_na() %>% 
  summarise(logRet_sum = sum(logRet),
            var = var(logRet)) 
    
  
weighted_close <- port_y_norm %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value)))
  
  
weighted_close_rebal <- port_rebal %>% 
  mutate(w_close = weight*close) %>%
  group_by(date) %>%
  summarise(port_value = sum(w_close)) %>% 
  ungroup() %>% 
  mutate(logRet = log(port_value/lag(port_value)))%>%
  group_by(date) %>% 
  summarise(port_value = sum(port_value))
  
portVal_plot <- weighted_close %>% 
  mutate(Year = year(date)) %>%
  group_by(Year) %>% 
  summarise(Sentiment = sum(port_value)) %>% 
  mutate(`Naive` = weighted_close_rebal$port_value) %>% 
  pivot_longer(-Year, names_to = "Portfolio", values_to = "Value") %>% 
  ggplot(aes(y = Value, x = Year, col = Portfolio)) + geom_line() +
  labs(x = "", y = "Porfolio Value", title = "Portfolio Comparison (Test & Train)", subtitle = "Training Window: 2014 - 2020") + theme_minimal()
                    
library(plotly)                       
ggplotly(portVal_plot)                              
   
  
```


